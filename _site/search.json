[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2023-10-27-SINDy_Challenges.html#installing-pysindy",
    "href": "posts/2023-10-27-SINDy_Challenges.html#installing-pysindy",
    "title": "SINDy - Challenge",
    "section": "Installing PySINDy",
    "text": "Installing PySINDy\nTo install the newest version of PySINDy, you must follow the instructions on the documentation. The version available through Conda is much older than the newest release. See https://pysindy.readthedocs.io/en/latest/index.html#installation.\n!pip install pysindy --quiet\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom sklearn.metrics import mean_squared_error\n\nimport os\n#print(os.getcwd())\n#os.chdir(\"../PhD/PhD/ANAET\")\nfrom pysindy.utils import lorenz, lorenz_control, enzyme\nimport pysindy as ps\n\n# bad code but allows us to ignore warnings\nimport warnings\nfrom scipy.integrate.odepack import ODEintWarning\nwarnings.simplefilter(\"ignore\", category=UserWarning)\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ODEintWarning)\n\n# Seed the random number generators for reproducibility\nnp.random.seed(100)\n\n# Initialize integrator keywords for solve_ivp to replicate the odeint defaults\nintegrator_keywords = {}\nintegrator_keywords['rtol'] = 1e-12\nintegrator_keywords['method'] = 'LSODA'\nintegrator_keywords['atol'] = 1e-12\n\nimport matplotlib as mpl\nmpl.rcParams[\"xtick.labelsize\"]=22\nmpl.rcParams[\"ytick.labelsize\"]=22\nmpl.rcParams[\"axes.labelsize\"]=26\nmpl.rcParams['mathtext.fontset'] = 'stix'\nmpl.rcParams['font.family'] = 'STIXGeneral'\n&lt;ipython-input-3-92171691bfc6&gt;:15: DeprecationWarning: Please use `ODEintWarning` from the `scipy.integrate` namespace, the `scipy.integrate.odepack` namespace is deprecated.\n  from scipy.integrate.odepack import ODEintWarning"
  },
  {
    "objectID": "posts/2023-10-27-SINDy_Challenges.html#mean-field-model",
    "href": "posts/2023-10-27-SINDy_Challenges.html#mean-field-model",
    "title": "SINDy - Challenge",
    "section": "Mean field model",
    "text": "Mean field model\nOne reduced-order model of flow past a cylinder is the coupled system of equations \\[\\begin{align}\n\\dot{x} &= \\mu x- \\omega y - xz,\\\\\n\\dot{y} &= \\mu y + \\omega x -yz,\\\\\n\\dot{z} & = -z + x^2 + y^2\n\\end{align}\\] where \\(\\mu\\) and \\(\\omega\\) are scalars. \\(\\omega\\) gives the frequency of oscillation.\nTasks: 1. Generate a training set of data using the initial conditions \\([0.01, 0.01, 0.1]\\), \\(\\omega=1\\) and \\(\\mu=0.1\\). Use SINDy to identify the model. 2. Now fit a SINDy model to data only when the model is on the stable limit cycle (the data is only oscillating). What happens to the model?\nmu = 0.1\nomega =1\nA=-1\nlam = 1\ndef cylinder_wake(t, xv):\n    x,y,z = xv\n    return [mu*x-omega*y+A*x*z,\n            omega*x + mu*y + A*y*z,\n            -z+x**2+y**2]\ny0 = [0.01, 0.01, 0.1]"
  },
  {
    "objectID": "posts/2023-05-05-Reinforcement_learning_Challenges.html#import-packages-and-setup-your-environment",
    "href": "posts/2023-05-05-Reinforcement_learning_Challenges.html#import-packages-and-setup-your-environment",
    "title": "Reinforcement learning introduction - Challenge",
    "section": "Import packages and setup your environment",
    "text": "Import packages and setup your environment\nimport numpy as np\nimport random\nFirst, we define the FrozenLake environment as a class, with methods for resetting the environment, taking actions, rendering the current state, and showing the current Q-table and policy.\nclass FrozenLake:\n    def __init__(self, size=4):\n        self.size = size\n        self.grid = np.zeros((4, 4), dtype=int) # spcifiy grid size please\n        self.start_state = (0, 0)\n        self.goal_state = (3, 3) # can you state the goal state in the grid (row, column)\n        self.hole_states = [(1, 1), (2, 3), (3, 0)]\n        for i, j in self.hole_states:\n            self.grid[i][j] = 1\n    \n    def reset(self):\n        self.current_state = self.start_state\n        return self.current_state\n    \n    def step(self, action): # could you please set the actions as numbers\n        i, j = self.current_state\n        if action == 0: # move up\n            i = max(i-1, 0)\n        elif action == 1: # move down\n            i = min(i+1, self.size-1)\n        elif action == 2: # move left\n            j = max(j-1, 0)\n        elif action == 3: # move right\n            j = min(j+1, self.size-1)\n        \n        self.current_state = (i, j)\n        \n        # what is your rewads?\n        if self.current_state == self.goal_state: \n            reward = 1 # winning reward?\n            done = True\n        elif self.current_state in self.hole_states:\n            reward = -1 # losing for hole states, reward?\n            done = True\n        else:\n            reward = 0 # else?\n            done = False\n        \n        return self.current_state, reward, done\n\n\n    def render(self):\n        print('\\n')\n        for i in range(self.size):\n            for j in range(self.size):\n                if self.grid[i][j] == 0:\n                    if (i, j) == self.current_state:\n                        print('S', end=' ')\n                    elif (i, j) == self.goal_state:\n                        print('G', end=' ')\n                    else:\n                        print('.', end=' ')\n                elif self.grid[i][j] == 1:\n                    if (i, j) == self.current_state:\n                        print('S', end=' ')\n                    else:\n                        print('X', end=' ')\n            print()\n        print()\n    \n    \n    def show_q_table(self, q_table):\n\n        print('-----------------------------------------------------------------')\n        print('Q-Table:')\n        print('-----------------------------------------------------------------')\n\n        for i in range(self.size):\n            for j in range(self.size):\n                if self.grid[i][j] == 0:\n                    print( '%.2f' % q_table[i][j][0], end='\\t')\n                    print('%.2f' % q_table[i][j][1], end='\\t')\n                    print('%.2f' % q_table[i][j][2], end='\\t')\n                    print('%.2f' % q_table[i][j][3])\n                else:\n                    print('NULL', end='\\t')\n                    print('NULL', end='\\t')\n                    print('NULL', end='\\t')\n                    print('NULL')\n            print()\n\n\n    # In one text line show the policy (the sequence of actions that agent take )\n    def show_policy(self, q_table):\n        print('\\n Policy:')\n        for i in range(self.size):\n            for j in range(self.size):\n                if self.grid[i][j] == 0:\n                    action = np.argmax(q_table[i][j])\n                    if action == 0:\n                        print('UP', end=' ')\n                    elif action == 1:\n                        print('DOWN', end=' ')\n                    elif action == 2:\n                        print('LEFT', end=' ')\n                    elif action == 3:\n                        print('RIGTH', end=' ')\n                else:\n                    print('STAY', end=' ')\nNext, we create an instance of the environment and initialize the Q-table with zeros.\nenv = FrozenLake()\nq_table = np.zeros((env.size, env.size, 4))"
  },
  {
    "objectID": "posts/2023-05-05-Reinforcement_learning_Challenges.html#hyperparameters",
    "href": "posts/2023-05-05-Reinforcement_learning_Challenges.html#hyperparameters",
    "title": "Reinforcement learning introduction - Challenge",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nWe then set some hyperparameters for the Q-learning algorithm, such as the number of episodes to run, the maximum number of steps per episode, the learning rate, the discount factor, the starting exploration rate (epsilon), the minimum exploration rate, and the rate at which epsilon decays over time.\n# Could you please set your hyperparameters?\nnum_episodes = 2000\nmax_steps_per_episode = 10\nlearning_rate = 0.05\ndiscount_factor = 0.99\n\nepsilon = 1.0\nmin_epsilon = 0.01\nepsilon_decay_rate = 0.001\nWe define an epsilon-greedy policy for selecting actions, which chooses a random action with probability epsilon or the greedy action (i.e., the action with the highest Q-value) with probability 1 - epsilon.\n\n\n\nScreen Shot 2023-04-16 at 2.11.38 PM.png\n\n\n# Define epsilon-greedy policy\ndef epsilon_greedy_policy(state):\n    if random.uniform(0, 1) &lt; epsilon:\n        return random.randint(0, 3)\n    else:\n        return np.argmax(q_table[state[0]][state[1]])\nWe train the agent by running a loop over the specified number of episodes. In each episode, we start by resetting the environment and selecting actions according to the epsilon-greedy policy. We then update the Q-values for the current state-action pair using the Q-learning update rule. Finally, we update the current state and repeat until the episode ends (either because the agent reaches the goal or exceeds the maximum number of steps).\nWe decay the exploration rate (epsilon) after each episode to gradually shift the agent from exploration to exploitation over time.\nPeriodically, we render the current state of the environment and display the current Q-table and policy for visualization.\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n    t = 0\n    while not done and t &lt; max_steps_per_episode:\n\n        action = epsilon_greedy_policy(state)\n\n        next_state, reward, done = env.step(action)\n\n        # what is missing to update the Q value?\n        #q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action])\n        q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action])\n\n        state = next_state\n\n        t += 1\n\n    epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay_rate))\n\n    # Show progress\n    if episode % 1000 == 0:\n        env.render()\n        env.show_q_table(q_table)\n        env.show_policy(q_table)\n. . . . \n. S . . \n. . . X \nX . . G \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.00    0.00    0.00    0.00\n0.00    -0.05   0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n0.00    0.00    0.00    0.00\nNULL    NULL    NULL    NULL\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nUP UP UP UP UP STAY UP UP UP UP UP STAY STAY UP UP UP \n\n. . . . \n. S . . \n. . . X \nX . . G \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.00    0.00    0.00    0.00\n0.00    -1.00   0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n0.00    0.00    0.00    -1.00\nNULL    NULL    NULL    NULL\n0.00    0.00    -0.64   0.00\n0.00    -0.19   0.00    0.00\n\n0.00    -0.74   0.00    0.00\n-0.26   0.00    0.00    0.00\n0.00    0.00    0.00    -0.05\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.00    0.00    -0.10   0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nUP UP UP UP UP STAY UP UP UP DOWN UP STAY STAY UP UP UP \n\nOnce training is complete, we test the agent by running a loop until the agent reaches the goal or exceeds the maximum number of steps. In each step, we select the greedy action (i.e., the action with the highest Q-value) and update the current state. We render the environment at each step for visualization.\n# Test agent\nstate = env.reset()\ndone = False\nwhile not done:\n    action = np.argmax(q_table[state[0]][state[1]])\n    next_state, reward, done = env.step(action)\n    env.render()\n    state = next_state\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G \n\n\n\nS . . . \n. X . . \n. . . X \nX . . G"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html",
    "href": "posts/2020-01-14-test-markdown-post.html",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Jekyll requires blog post files to be named according to the following format:\nYEAR-MONTH-DAY-filename.md\nWhere YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files.\nThe first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above.\n\n\n\nYou can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule:\n\n\n\n\nHere‚Äôs a list:\n\nitem 1\nitem 2\n\nAnd a numbered list:\n\nitem 1\nitem 2\n\n\n\n\n\nThis is a quotation\n\n{% include alert.html text=‚ÄúYou can include alert boxes‚Äù %}\n‚Ä¶and‚Ä¶\n{% include info.html text=‚ÄúYou can include info boxes‚Äù %}\n\n\n\n\n\n\n\nYou can format text and code per usual\nGeneral preformatted text:\n# Do a thing\ndo_thing()\nPython code and output:\n# Prints '2'\nprint(1+1)\n2\nFormatting text as shell commands:\necho \"hello world\"\n./some_script.sh --option \"value\"\nwget https://example.com/cat_photo1.png\nFormatting text as YAML:\nkey: value\n- another_key: \"another value\"\n\n\n\n\n\n\nColumn 1\nColumn 2\n\n\n\n\nA thing\nAnother thing\n\n\n\n\n\n\n{% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %}"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#basic-setup",
    "href": "posts/2020-01-14-test-markdown-post.html#basic-setup",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Jekyll requires blog post files to be named according to the following format:\nYEAR-MONTH-DAY-filename.md\nWhere YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files.\nThe first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above."
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#basic-formatting",
    "href": "posts/2020-01-14-test-markdown-post.html#basic-formatting",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule:"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#lists",
    "href": "posts/2020-01-14-test-markdown-post.html#lists",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Here‚Äôs a list:\n\nitem 1\nitem 2\n\nAnd a numbered list:\n\nitem 1\nitem 2"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#boxes-and-stuff",
    "href": "posts/2020-01-14-test-markdown-post.html#boxes-and-stuff",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "This is a quotation\n\n{% include alert.html text=‚ÄúYou can include alert boxes‚Äù %}\n‚Ä¶and‚Ä¶\n{% include info.html text=‚ÄúYou can include info boxes‚Äù %}"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#code",
    "href": "posts/2020-01-14-test-markdown-post.html#code",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "You can format text and code per usual\nGeneral preformatted text:\n# Do a thing\ndo_thing()\nPython code and output:\n# Prints '2'\nprint(1+1)\n2\nFormatting text as shell commands:\necho \"hello world\"\n./some_script.sh --option \"value\"\nwget https://example.com/cat_photo1.png\nFormatting text as YAML:\nkey: value\n- another_key: \"another value\""
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#tables",
    "href": "posts/2020-01-14-test-markdown-post.html#tables",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Column 1\nColumn 2\n\n\n\n\nA thing\nAnother thing"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#tweetcards",
    "href": "posts/2020-01-14-test-markdown-post.html#tweetcards",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "{% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %}"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html#footnotes",
    "href": "posts/2020-01-14-test-markdown-post.html#footnotes",
    "title": "An Example Markdown Post",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the footnote.‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sciml_quarto_test",
    "section": "",
    "text": "Reinforcement learning introduction - Solution\n\n\n\n\n\n\nworkshop\n\n\nreinforcement_learning\n\n\n\nReinforcement learning introduction - Frozen Lake example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSINDy - Challenge\n\n\n\n\n\n\nworkshop\n\n\nSINDy\n\n\n\nSINDy workshop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement learning introduction - Challenge\n\n\n\n\n\n\nworkshop\n\n\nreinforcement_learning\n\n\n\nReinforcement learning introduction - Frozen Lake example\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFastpages Notebook Blog Post\n\n\n\n\n\n\njupyter\n\n\n\nA tutorial of fastpages for Jupyter notebooks.\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAn Example Markdown Post\n\n\n\n\n\n\nmarkdown\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\nJan 14, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-02-20-test.html",
    "href": "posts/2020-02-20-test.html",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "This notebook is a demonstration of some of capabilities of fastpages with notebooks.\nWith fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts!\n\n\nThe first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:\n# \"My Title\"\n&gt; \"Awesome summary\"\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\n\nSetting toc: true will automatically generate a table of contents\nSetting badges: true will automatically include GitHub and Google Colab links to your notebook.\nSetting comments: true will enable commenting on your blog post, powered by utterances.\n\nThe title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README.\n\n\n\nA #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post.\nA #hide_input comment at the top of any code cell will only hide the input of that cell.\n\n\nThe comment #hide_input was used to hide the code that produced this.\n\n\nput a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\nput a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:\n\n\nCode\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\n\nplace a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it:\n\n#collapse-output\nprint('The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.')\n\nThe comment #collapse-output was used to collapse the output of this cell by default but you can expand it.\n\n\n\n\n\nCharts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook.\n\n\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(df).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\n\n\n\n\nalt.Chart(df).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    alt.X('Rotten_Tomatoes_Rating', type='quantitative'),\n    alt.Y('IMDB_Rating', type='quantitative', axis=alt.Axis(minExtent=30)),\n#     y=alt.Y('IMDB_Rating:Q', ), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\nYou can display tables per the usual way in your blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'Distributor', 'MPAA_Rating', 'IMDB_Rating', 'Rotten_Tomatoes_Rating']].head()\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nDistributor\nMPAA_Rating\nIMDB_Rating\nRotten_Tomatoes_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\nGramercy\nR\n6.1\nNaN\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\nStrand\nR\n6.9\nNaN\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\nLionsgate\nNone\n6.8\nNaN\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nFine Line\nNone\nNaN\n13.0\n\n\n4\nSlam\n1087521.0\n1000000.0\nTrimark\nR\n3.4\n62.0\n\n\n\n\n\n\n\n\n\n\n\n\nYou can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax:\n![](my_icons/fastai_logo.png)\n\n\n\n\nRemote images can be included with the following markdown syntax:\n![](https://image.flaticon.com/icons/svg/36/36686.svg)\n\n\n\n\nAnimated Gifs work, too!\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)\n\n\n\n\nYou can include captions with markdown images like this:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")"
  },
  {
    "objectID": "posts/2020-02-20-test.html#front-matter",
    "href": "posts/2020-02-20-test.html#front-matter",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:\n# \"My Title\"\n&gt; \"Awesome summary\"\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\n\nSetting toc: true will automatically generate a table of contents\nSetting badges: true will automatically include GitHub and Google Colab links to your notebook.\nSetting comments: true will enable commenting on your blog post, powered by utterances.\n\nThe title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README."
  },
  {
    "objectID": "posts/2020-02-20-test.html#markdown-shortcuts",
    "href": "posts/2020-02-20-test.html#markdown-shortcuts",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post.\nA #hide_input comment at the top of any code cell will only hide the input of that cell.\n\n\nThe comment #hide_input was used to hide the code that produced this.\n\n\nput a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\nput a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:\n\n\nCode\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\n\nplace a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it:\n\n#collapse-output\nprint('The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.')\n\nThe comment #collapse-output was used to collapse the output of this cell by default but you can expand it."
  },
  {
    "objectID": "posts/2020-02-20-test.html#interactive-charts-with-altair",
    "href": "posts/2020-02-20-test.html#interactive-charts-with-altair",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook.\n\n\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(df).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\n\n\n\n\nalt.Chart(df).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    alt.X('Rotten_Tomatoes_Rating', type='quantitative'),\n    alt.Y('IMDB_Rating', type='quantitative', axis=alt.Axis(minExtent=30)),\n#     y=alt.Y('IMDB_Rating:Q', ), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)"
  },
  {
    "objectID": "posts/2020-02-20-test.html#data-tables",
    "href": "posts/2020-02-20-test.html#data-tables",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "You can display tables per the usual way in your blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'Distributor', 'MPAA_Rating', 'IMDB_Rating', 'Rotten_Tomatoes_Rating']].head()\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nDistributor\nMPAA_Rating\nIMDB_Rating\nRotten_Tomatoes_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\nGramercy\nR\n6.1\nNaN\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\nStrand\nR\n6.9\nNaN\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\nLionsgate\nNone\n6.8\nNaN\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nFine Line\nNone\nNaN\n13.0\n\n\n4\nSlam\n1087521.0\n1000000.0\nTrimark\nR\n3.4\n62.0"
  },
  {
    "objectID": "posts/2020-02-20-test.html#images",
    "href": "posts/2020-02-20-test.html#images",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax:\n![](my_icons/fastai_logo.png)\n\n\n\n\nRemote images can be included with the following markdown syntax:\n![](https://image.flaticon.com/icons/svg/36/36686.svg)\n\n\n\n\nAnimated Gifs work, too!\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)\n\n\n\n\nYou can include captions with markdown images like this:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")"
  },
  {
    "objectID": "posts/2020-02-20-test.html#github-flavored-emojis",
    "href": "posts/2020-02-20-test.html#github-flavored-emojis",
    "title": "Fastpages Notebook Blog Post",
    "section": "GitHub Flavored Emojis",
    "text": "GitHub Flavored Emojis\nTyping I give this post two :+1:! will render this:\nI give this post two :+1:!"
  },
  {
    "objectID": "posts/2020-02-20-test.html#tweetcards",
    "href": "posts/2020-02-20-test.html#tweetcards",
    "title": "Fastpages Notebook Blog Post",
    "section": "Tweetcards",
    "text": "Tweetcards\nTyping &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20"
  },
  {
    "objectID": "posts/2020-02-20-test.html#youtube-videos",
    "href": "posts/2020-02-20-test.html#youtube-videos",
    "title": "Fastpages Notebook Blog Post",
    "section": "Youtube Videos",
    "text": "Youtube Videos\nTyping &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this:"
  },
  {
    "objectID": "posts/2020-02-20-test.html#boxes-callouts",
    "href": "posts/2020-02-20-test.html#boxes-callouts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Boxes / Callouts",
    "text": "Boxes / Callouts\nTyping &gt; Warning: There will be no second warning! will render this:\n\n\n\n\n\n\nWarning\n\n\n\nThere will be no second warning!\n\n\nTyping &gt; Important: Pay attention! It's important. will render this:\n\n\n\n\n\n\nImportant\n\n\n\nPay attention! It‚Äôs important.\n\n\nTyping &gt; Tip: This is my tip. will render this:\n\n\n\n\n\n\nTip\n\n\n\nThis is my tip.\n\n\nTyping &gt; Note: Take note of this. will render this:\n\n\n\n\n\n\nNote\n\n\n\nTake note of this.\n\n\nTyping &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs:\n\n\n\n\n\n\nNote\n\n\n\nA doc link to an example website: fast.ai should also work fine."
  },
  {
    "objectID": "posts/2020-02-20-test.html#footnotes",
    "href": "posts/2020-02-20-test.html#footnotes",
    "title": "Fastpages Notebook Blog Post",
    "section": "Footnotes",
    "text": "Footnotes\nYou can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this:\n{% raw %}For example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ 'This is the footnote.' | fndetail: 1 }}\n{{ 'This is the other footnote. You can even have a [link](www.github.com)!' | fndetail: 2 }}{% endraw %}\nFor example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ ‚ÄòThis is the footnote.‚Äô | fndetail: 1 }} {{ ‚ÄòThis is the other footnote. You can even have a link!‚Äô | fndetail: 2 }}"
  },
  {
    "objectID": "posts/2023-05-05-Reinforcement_learning_Solution.html#import-packages-and-setup-your-environment",
    "href": "posts/2023-05-05-Reinforcement_learning_Solution.html#import-packages-and-setup-your-environment",
    "title": "Reinforcement learning introduction - Solution",
    "section": "Import packages and setup your environment",
    "text": "Import packages and setup your environment\nThis code imports the FrozenLake environment from the OpenAI Gym library and creates an instance of the environment.\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Create the FrozenLake environment\nenv = gym.make('FrozenLake-v1')\nThis code initializes the Q-table to zeros. The Q-table is a matrix where the rows represent the possible states of the environment and the columns represent the possible actions that the agent can take.\n# Initialize the Q-table to zeros\nQ = np.zeros([env.observation_space.n, env.action_space.n])"
  },
  {
    "objectID": "posts/2023-05-05-Reinforcement_learning_Solution.html#hyperparameters",
    "href": "posts/2023-05-05-Reinforcement_learning_Solution.html#hyperparameters",
    "title": "Reinforcement learning introduction - Solution",
    "section": "Hyperparameters üìà",
    "text": "Hyperparameters üìà\n# Set hyperparameters\nlr = 0.8  # learning rate\ngamma = 0.95  # discount factor\nnum_episodes = 2000  # number of training episodes"
  },
  {
    "objectID": "posts/2023-05-05-Reinforcement_learning_Solution.html#train-your-agent",
    "href": "posts/2023-05-05-Reinforcement_learning_Solution.html#train-your-agent",
    "title": "Reinforcement learning introduction - Solution",
    "section": "TRAIN YOUR AGENT ü§ñ ‚ùÑ",
    "text": "TRAIN YOUR AGENT ü§ñ ‚ùÑ\nThis code trains the agent using Q-learning. During training, the agent interacts with the environment by selecting actions based on the Q-table and updating the Q-table based on the observed reward. The hyperparameters lr and gamma control how much the agent values immediate rewards versus future rewards. The num_episodes parameter controls how many times the agent interacts with the environment.\n# Keep track of the total reward for each episode \nrewards = np.zeros(num_episodes)\n\n# Train the agent using Q-learning\nfor i in range(num_episodes):\n    # Reset the environment for each episode\n    s = env.reset()\n    done = False\n    while not done:\n        # Choose an action based on the Q-table, with some random noise\n        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i+1)))\n        \n        # Take the chosen action and observe the next state and reward\n        s_new, r, done, _ = env.step(a)\n        \n        # Update the Q-table based on the observed reward\n        Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s_new,:]) - Q[s,a])\n        \n        \n        # Add the reward to the total reward for the episode\n        rewards[i] += r\n        \n        # Set the current state to the next state\n        s = s_new"
  },
  {
    "objectID": "posts/2023-05-05-Reinforcement_learning_Solution.html#test-your-agent",
    "href": "posts/2023-05-05-Reinforcement_learning_Solution.html#test-your-agent",
    "title": "Reinforcement learning introduction - Solution",
    "section": "TEST YOUR AGENT üß™",
    "text": "TEST YOUR AGENT üß™\nThis code tests the agent on 100 episodes after training. During testing, the agent chooses actions based on the Q-table and tries to reach the goal state. The code keeps track of the number of successful episodes (where the agent reaches the goal state) and prints the success rate at the end.\n# Test the agent on 100 episodes \nnum_successes = 0\nfor i in range(100):\n    s = env.reset()\n    done = False\n    while not done:\n        # Choose an action based on the Q-table\n        a = np.argmax(Q[s,:])\n        s, r, done, _ = env.step(a)\n    if r == 1:\n        num_successes += 1\n\n# Print the success rate\nprint(\"Success rate:\", num_successes/100)\nSuccess rate: 0.54"
  },
  {
    "objectID": "posts/2023-05-05-Reinforcement_learning_Solution.html#hyperparameters-1",
    "href": "posts/2023-05-05-Reinforcement_learning_Solution.html#hyperparameters-1",
    "title": "Reinforcement learning introduction - Solution",
    "section": "Hyperparameters üìà",
    "text": "Hyperparameters üìà\nWe then set some hyperparameters for the Q-learning algorithm, such as the number of episodes to run, the maximum number of steps per episode, the learning rate, the discount factor, the starting exploration rate (epsilon), the minimum exploration rate, and the rate at which epsilon decays over time.\n# Set hyperparameters\nnum_episodes = 10000\nmax_steps_per_episode = 100\nlearning_rate = 0.1\ndiscount_factor = 0.99\nepsilon = 1.0\nmin_epsilon = 0.01\nepsilon_decay_rate = 0.001\nWe define an epsilon-greedy policy for selecting actions, which chooses a random action with probability epsilon or the greedy action (i.e., the action with the highest Q-value) with probability 1 - epsilon.\n\n\n\nScreen Shot 2023-04-16 at 2.11.38 PM.png\n\n\n# Define epsilon-greedy policy\ndef epsilon_greedy_policy(state):\n    if random.uniform(0, 1) &lt; epsilon:\n        return random.randint(0, 3)\n    else:\n        return np.argmax(q_table[state[0]][state[1]])\nWe train the agent by running a loop over the specified number of episodes. In each episode, we start by resetting the environment and selecting actions according to the epsilon-greedy policy. We then update the Q-values for the current state-action pair using the Q-learning update rule. Finally, we update the current state and repeat until the episode ends (either because the agent reaches the goal or exceeds the maximum number of steps).\nWe decay the exploration rate (epsilon) after each episode to gradually shift the agent from exploration to exploitation over time.\nPeriodically, we render the current state of the environment and display the current Q-table and policy for visualization.\n# Train agent\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n    t = 0\n    while not done and t &lt; max_steps_per_episode:\n        action = epsilon_greedy_policy(state)\n        next_state, reward, done = env.step(action)\n        q_table[state[0]][state[1]][action] += learning_rate * \\\n            (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action])\n        state = next_state\n        t += 1\n    epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay_rate))\n\n    # Show progress\n    if episode % 1000 == 0:\n        env.render()\n        env.show_q_table(q_table)\n        env.show_policy(q_table)\n. . . . \n. S . . \n. . . X \nX . . G \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n0.00    0.00    0.00    -0.10\nNULL    NULL    NULL    NULL\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nUP UP UP UP UP STAY UP UP UP UP UP STAY STAY UP UP UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.84    0.85    0.96    0.82\n\n0.94    0.58    0.85    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.65    -0.95   0.97    0.80\n\n0.47    -1.00   0.32    0.81\n-0.95   0.72    0.45    0.98\n0.97    0.99    0.96    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.58    0.71    -0.81   0.98\n0.98    0.99    0.94    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.88    0.91    0.96    0.90\n\n0.94    0.69    0.88    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.91\n-0.95   0.79    0.66    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.65    0.84    -0.88   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.89    0.91    0.96    0.90\n\n0.94    0.73    0.89    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.92\n-0.96   0.81    0.69    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.68    0.84    -0.89   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.89    0.91    0.96    0.91\n\n0.94    0.73    0.89    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.92\n-0.96   0.81    0.69    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.68    0.84    -0.89   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.89    0.91    0.96    0.91\n\n0.94    0.73    0.89    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.92\n-0.96   0.81    0.69    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.68    0.84    -0.89   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.89    0.91    0.96    0.91\n\n0.94    0.73    0.89    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.92\n-0.96   0.81    0.69    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.68    0.84    -0.89   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.89    0.91    0.96    0.91\n\n0.94    0.73    0.89    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.92\n-0.96   0.81    0.69    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.68    0.84    -0.89   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.89    0.91    0.96    0.91\n\n0.94    0.73    0.89    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.92\n-0.96   0.81    0.69    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.68    0.84    -0.89   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \n\n. . . . \n. X . . \n. . . X \nX . . S \n\n-----------------------------------------------------------------\nQ-Table:\n-----------------------------------------------------------------\n0.94    0.93    0.94    0.95\n0.95    -1.00   0.94    0.96\n0.96    0.97    0.95    0.95\n0.89    0.91    0.96    0.91\n\n0.94    0.73    0.89    -1.00\nNULL    NULL    NULL    NULL\n0.96    0.98    -1.00   0.96\n0.81    -0.96   0.97    0.90\n\n0.56    -1.00   0.37    0.92\n-0.96   0.81    0.69    0.98\n0.97    0.99    0.97    -1.00\nNULL    NULL    NULL    NULL\n\nNULL    NULL    NULL    NULL\n0.68    0.84    -0.89   0.99\n0.98    0.99    0.98    1.00\n0.00    0.00    0.00    0.00\n\n\nPolicy:\nRIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP \nOnce training is complete, we test the agent by running a loop until the agent reaches the goal or exceeds the maximum number of steps. In each step, we select the greedy action (i.e., the action with the highest Q-value) and update the current state. We render the environment at each step for visualization.\n\n# Test agent\nstate = env.reset()\ndone = False\nwhile not done:\n    action = np.argmax(q_table[state[0]][state[1]])\n    next_state, reward, done = env.step(action)\n    env.render()\n    state = next_state\n. S . . \n. X . . \n. . . X \nX . . G \n\n\n\n. . S . \n. X . . \n. . . X \nX . . G \n\n\n\n. . . . \n. X S . \n. . . X \nX . . G \n\n\n\n. . . . \n. X . . \n. . S X \nX . . G \n\n\n\n. . . . \n. X . . \n. . . X \nX . S G \n\n\n\n. . . . \n. X . . \n. . . X \nX . . S"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]